<html><head>
        <!-- Generated by Ddoc from ../../../.dub/packages/dstats-1.0.3/dstats/source/dstats/infotheory.d -->
        <META http-equiv="content-type" content="text/html; charset=utf-8">
        <title>dstats.infotheory</title>
        </head><body>
        <h1>dstats.infotheory</h1>
Basic information theory.  Joint entropy, mutual information, conditional
 mutual information.  This module uses the base 2 definition of these
 quantities, i.e, entropy, mutual info, etc. are output in bits.
<br><br>
<b>Author:</b><br>
David Simcha<br><br>

<dl><dt><big><a name="entropyCounts"></a>double <u>entropyCounts</u>(T)(T <i>data</i>) if (isForwardRange!T &amp;&amp; doubleInput!T);
</big></dt>
<dd>This function calculates the Shannon entropy of a forward range that is
 treated as frequency counts of a set of discrete observations.
<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>double</font> uniform3 = <u>entropyCounts</u>([4, 4, 4]);
<font color=blue>assert</font>(approxEqual(uniform3, log2(3)));
<font color=blue>double</font> uniform4 = <u>entropyCounts</u>([5, 5, 5, 5]);
<font color=blue>assert</font>(approxEqual(uniform4, 2));
</pre>
<br><br>

</dd>
<dt><big><a name="joint"></a>Joint!(FlattenType!T) <u>joint</u>(T...)(T <i>args</i>);
</big></dt>
<dd>Bind a set of ranges together to represent a <u>joint</u> probability distribution.
<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>auto</font> foo = [1,2,3,1,1];
<font color=blue>auto</font> bar = [2,4,6,2,2];
<font color=blue>auto</font> e = entropy(<u>joint</u>(foo, bar));  <font color=green>// Calculate joint entropy of foo, bar.
</font></pre>
<br><br>

</dd>
<dt><big><a name="Joint"></a>struct <u>Joint</u>(T...);
</big></dt>
<dd>Iterate over a set of ranges by value in lockstep and return an ObsEnt,
 which is used internally by entropy functions on each iteration.<br><br>

</dd>
<dt><big><a name="entropy"></a>double <u>entropy</u>(T)(T <i>data</i>) if (isIterable!T);
</big></dt>
<dd>Calculates the joint <u>entropy</u> of a set of observations.  Each input range
 represents a vector of observations. If only one range is given, this reduces
 to the plain old <u>entropy</u>.  Input range must have a length.
<br><br>
<b>Note:</b><br>
This function specializes if ElementType!(T) is a byte, ubyte, or
 char, resulting in a much faster <u>entropy</u> calculation.  When possible, try
 to provide <i>data</i> in the form of a byte, ubyte, or char.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>int</font>[] foo = [1, 1, 1, 2, 2, 2, 3, 3, 3];
<font color=blue>double</font> entropyFoo = <u>entropy</u>(foo);  <font color=green>// Plain old entropy of foo.
</font><font color=blue>assert</font>(approxEqual(entropyFoo, log2(3)));
<font color=blue>int</font>[] bar = [1, 2, 3, 1, 2, 3, 1, 2, 3];
<font color=blue>double</font> HFooBar = <u>entropy</u>(joint(foo, bar));  <font color=green>// Joint entropy of foo and bar.
</font><font color=blue>assert</font>(approxEqual(HFooBar, log2(9)));
</pre>
<br><br>

</dd>
<dt><big><a name="condEntropy"></a>double <u>condEntropy</u>(T, U)(T <i>data</i>, U <i>cond</i>) if (isInputRange!T &amp;&amp; isInputRange!U);
</big></dt>
<dd>Calculate the conditional entropy H(<i>data</i> | <i>cond</i>).<br><br>

</dd>
<dt><big><a name="mutualInfo"></a>double <u>mutualInfo</u>(T, U)(T <i>x</i>, U <i>y</i>) if (isInputRange!T &amp;&amp; isInputRange!U);
</big></dt>
<dd>Calculates the mutual information of two vectors of discrete observations.<br><br>

</dd>
<dt><big><a name="mutualInfoTable"></a>double <u>mutualInfoTable</u>(T...)(T <i>table</i>);
</big></dt>
<dd>Calculates the mutual information of a contingency <i>table</i> representing a joint
discrete probability distribution.  Takes a set of finite forward ranges,
one for each column in the contingency <i>table</i>.  These can be expressed either as
a tuple of ranges or a range of ranges.<br><br>

</dd>
<dt><big><a name="condMutualInfo"></a>double <u>condMutualInfo</u>(T, U, V)(T <i>x</i>, U <i>y</i>, V <i>z</i>);
</big></dt>
<dd>Calculates the conditional mutual information I(<i>x</i>, <i>y</i> | <i>z</i>) from a set of
observations.<br><br>

</dd>
<dt><big><a name="entropySorted"></a>double <u>entropySorted</u>(alias compFun = "a == b", T)(T <i>data</i>) if (isInputRange!T);
</big></dt>
<dd>Calculates the entropy of any old input range of observations more quickly
 than entropy(), provided that all equal values are adjacent.  If the input
 is sorted by more than one key, i.e. structs, the result will be the joint
 entropy of all of the keys.  The compFun alias will be used to compare
 adjacent elements and determine how many instances of each value exist.<br><br>

</dd>
<dt><big><a name="DenseInfoTheory"></a>struct <u>DenseInfoTheory</u>;
</big></dt>
<dd>Much faster implementations of information theory functions for the special
but common case where all observations are integers on the range [0, nBin).
This is the case, for example, when the observations have been previously
binned using, for example, dstats.base.frqBin().
<br><br>
Note that, due to the optimizations used, joint() cannot be used with
the member functions of this struct, except entropy().
<br><br>

For those looking for hard numbers, this seems to be on the order of 10x
faster than the generic implementations according to my quick and dirty
benchmarks.<br><br>

<dl><dt><big><a name="DenseInfoTheory.this"></a>this(uint <i>nBin</i>);
</big></dt>
<dd>Constructs a DenseInfoTheory object for <i>nBin</i> bins.  The values taken by
    each observation must then be on the interval [0, <i>nBin</i>).<br><br>

</dd>
<dt><big><a name="DenseInfoTheory.entropy"></a>double <u>entropy</u>(R)(R <i>range</i>) if (isIterable!R);
</big></dt>
<dd>Computes the <u>entropy</u> of a set of observations.  Note that, for this
    function, the joint() function can be used to compute joint entropies
    as long as each individual <i>range</i> contains only integers on [0, nBin).<br><br>

</dd>
<dt><big><a name="DenseInfoTheory.mutualInfo"></a>double <u>mutualInfo</u>(R1, R2)(R1 <i>x</i>, R2 <i>y</i>) if (isIterable!R1 &amp;&amp; isIterable!R2);
</big></dt>
<dd>I(<i>x</i>; <i>y</i>)<br><br>

</dd>
<dt><big><a name="DenseInfoTheory.mutualInfoPval"></a>double <u>mutualInfoPval</u>(double <i>mutualInfo</i>, double <i>n</i>);
</big></dt>
<dd>Calculates the P-value for I(X; Y) assuming x and y both have supports
    of [0, nBin).  The P-value is calculated using a Chi-Square approximation.
    It is asymptotically correct, but is approximate for finite sample size.
<br><br>
<b>Parameters:</b><br>
<br><br>
<b>mutualInfo:</b><br>
I(x; y), in bits
<br><br>
<b>n:</b><br>
The number of samples used to calculate I(x; y)<br><br>

</dd>
<dt><big><a name="DenseInfoTheory.condEntropy"></a>double <u>condEntropy</u>(R1, R2)(R1 <i>x</i>, R2 <i>y</i>) if (isIterable!R1 &amp;&amp; isIterable!R2);
</big></dt>
<dd>H(X | Y)<br><br>

</dd>
<dt><big><a name="DenseInfoTheory.condMutualInfo"></a>double <u>condMutualInfo</u>(R1, R2, R3)(R1 <i>x</i>, R2 <i>y</i>, R3 <i>z</i>) if (allSatisfy!(isIterable, R1, R2, R3));
</big></dt>
<dd>I(X; Y | Z)<br><br>

</dd>
</dl>
</dd>
</dl>

        <hr><small>Page generated by <a href="http://dlang.org/ddoc.html">Ddoc</a>. </small>
        </body></html>
