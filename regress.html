<html><head>
        <!-- Generated by Ddoc from ../../../.dub/packages/dstats-1.0.3/dstats/source/dstats/regress.d -->
        <META http-equiv="content-type" content="text/html; charset=utf-8">
        <title>dstats.regress</title>
        </head><body>
        <h1>dstats.regress</h1>
A module for performing linear regression.  This module has an unusual
 interface, as it is range-based instead of matrix based. Values for
 independent variables are provided as either a tuple or a range of ranges.
 This means that one can use, for example, map, to fit high order models and
 lazily evaluate certain values.  (For details, see examples below.)
<br><br>
<b>Author:</b><br>
David Simcha<br><br>

<dl><dt><big><a name="PowMap"></a>struct <u>PowMap</u>(ExpType, T) if (isForwardRange!T);
</big></dt>
<dd><br><br>
</dd>
<dt><big><a name="powMap"></a>PowMap!(ExpType, T) <u>powMap</u>(ExpType, T)(T <i>range</i>, ExpType <i>exponent</i>);
</big></dt>
<dd>Maps a forward <i>range</i> to a power determined at runtime.  ExpType is the type
 of the <i>exponent</i>.  Using an int is faster than using a double, but obviously
 less flexible.<br><br>

</dd>
<dt><big><a name="RegressRes"></a>struct <u>RegressRes</u>;
</big></dt>
<dd>Struct that holds the results of a linear regression.  It's a plain old
 data struct.<br><br>

<dl><dt><big><a name="RegressRes.betas"></a>double[] <u>betas</u>;
</big></dt>
<dd>The coefficients, one for each range in X.  These will be in the order
 that the X ranges were passed in.<br><br>

</dd>
<dt><big><a name="RegressRes.stdErr"></a>double[] <u>stdErr</u>;
</big></dt>
<dd>The standard error terms of the X ranges passed in.<br><br>

</dd>
<dt><big><a name="RegressRes.lowerBound"></a>double[] <u>lowerBound</u>;
</big></dt>
<dd>The lower confidence bounds of the beta terms, at the confidence level
 specificied.  (Default 0.95).<br><br>

</dd>
<dt><big><a name="RegressRes.upperBound"></a>double[] <u>upperBound</u>;
</big></dt>
<dd>The upper confidence bounds of the beta terms, at the confidence level
 specificied.  (Default 0.95).<br><br>

</dd>
<dt><big><a name="RegressRes.p"></a>double[] <u>p</u>;
</big></dt>
<dd>The P-value for the alternative that the corresponding beta value is
 different from zero against the <b>null</b> that it is equal to zero.<br><br>

</dd>
<dt><big><a name="RegressRes.R2"></a>double <u>R2</u>;
</big></dt>
<dd>The coefficient of determination.<br><br>

</dd>
<dt><big><a name="RegressRes.adjustedR2"></a>double <u>adjustedR2</u>;
</big></dt>
<dd>The adjusted coefficient of determination.<br><br>

</dd>
<dt><big><a name="RegressRes.residualError"></a>double <u>residualError</u>;
</big></dt>
<dd>The root mean square of the residuals.<br><br>

</dd>
<dt><big><a name="RegressRes.overallP"></a>double <u>overallP</u>;
</big></dt>
<dd>The P-value for the model as a whole.  Based on an F-statistic.  The
 <b>null</b> here is that the model has no predictive value, the alternative
 is that it does.<br><br>

</dd>
<dt><big><a name="RegressRes.toString"></a>string <u>toString</u>();
</big></dt>
<dd>Print out the results in the default format.<br><br>

</dd>
</dl>
</dd>
<dt><big><a name="Residuals"></a>struct <u>Residuals</u>(F, U, T...);
</big></dt>
<dd>Forward Range for holding the residuals from a regression analysis.<br><br>

</dd>
<dt><big><a name="residuals"></a>Residuals!(F, U, T) <u>residuals</u>(F, U, T...)(F[] <i>betas</i>, U <i>Y</i>, T <i>X</i>) if (isFloatingPoint!F &amp;&amp; isForwardRange!U &amp;&amp; allSatisfy!(isForwardRange, T));
</big></dt>
<dd>Given the beta coefficients from a linear regression, and <i>X</i> and <i>Y</i> values,
 returns a range that lazily computes the <u>residuals</u>.<br><br>

</dd>
<dt><big><a name="linearRegressBeta"></a>double[] <u>linearRegressBeta</u>(U, T...)(U <i>Y</i>, T <i>XIn</i>) if (doubleInput!U);
</big></dt>
<dd>Perform a linear regression and return just the beta values.  The advantages
to just returning the beta values are that it's faster and that each range
needs to be iterated over only once, and thus can be just an input range.
The beta values are returned such that the smallest index corresponds to
the leftmost element of X.  X can be either a tuple or a range of input
ranges.  <i>Y</i> must be an input range.
<br><br>
If, after all X variables are passed in, a numeric type is passed as the last
parameter, this is treated as a ridge parameter and ridge regression is
performed.  Ridge regression is a form of regression that penalizes the L2 norm
of the beta vector and therefore results in more parsimonious models.
However, it makes statistical inference such as that supported by
linearRegress() difficult to impossible.  Therefore, linearRegress() doesn't
support ridges.
<br><br>

If no ridge parameter is passed, or equivalently if the ridge parameter is
zero, then ordinary least squares regression is performed.

<br><br>
<b>Notes:</b><br>
The X ranges are traversed in lockstep, but the traversal is stopped
at the end of the shortest one.  Therefore, using infinite ranges is safe.
For example, using repeat(1) to get an intercept term works.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//www.mathworks.com/help/toolbox/stats/ridge.html
<br><br>

Venables, W. N. &amp; Ripley, B. D. (2002) Modern Applied Statistics with S.
Fourth Edition. Springer, New York. ISBN 0-387-95457-0
(This is the citation for the MASS R package.)

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>int</font>[] nBeers = [8,6,7,5,3,0,9];
<font color=blue>int</font>[] nCoffees = [3,6,2,4,3,6,8];
<font color=blue>int</font>[] musicVolume = [3,1,4,1,5,9,2];
<font color=blue>int</font>[] programmingSkill = [2,7,1,8,2,8,1];
<font color=blue>double</font>[] betas = <u>linearRegressBeta</u>(programmingSkill, repeat(1), nBeers, nCoffees,
    musicVolume, map!<font color=red>"a * a"</font>(musicVolume));

<font color=green>// Now throw in a ridge parameter of 2.5.
</font><font color=blue>double</font>[] ridgeBetas = <u>linearRegressBeta</u>(programmingSkill, repeat(1), nBeers,
    nCoffees, musicVolume, map!<font color=red>"a * a"</font>(musicVolume), 2.5);
</pre>
<br><br>

</dd>
<dt><big><a name="linearRegressBetaBuf"></a>double[] <u>linearRegressBetaBuf</u>(U, TRidge...)(double[] <i>buf</i>, U <i>Y</i>, TRidge <i>XRidge</i>) if (doubleInput!U);
</big></dt>
<dd>Same as linearRegressBeta, but allows the user to specify a buffer for
the beta terms.  If the buffer is too short, a new one is allocated.
Otherwise, the results are returned in the user-provided buffer.<br><br>

</dd>
<dt><big><a name="linearRegress"></a>RegressRes <u>linearRegress</u>(U, TC...)(U <i>Y</i>, TC <i>input</i>);
</big></dt>
<dd>Perform a linear regression as in linearRegressBeta, but return a
RegressRes with useful stuff for statistical inference.  If the last element
of <i>input</i> is a real, this is used to specify the confidence intervals to
be calculated.  Otherwise, the default of 0.95 is used.  The rest of <i>input</i>
should be the elements of X.
<br><br>
When using this function, which provides several useful statistics useful
for inference, each range must be traversed twice.  This means:
<br><br>

1.  They have to be forward ranges, not <i>input</i> ranges.
<br><br>

2.  If you have a large amount of data and you're mapping it to some
    expensive function, you may want to do this eagerly instead of lazily.

<br><br>
<b>Notes:</b><br>
The X ranges are traversed in lockstep, but the traversal is stopped
at the end of the shortest one.  Therefore, using infinite ranges is safe.
For example, using repeat(1) to get an intercept term works.
<br><br>

If the confidence interval specified is exactly 0, this is treated as a
special case and confidence interval calculation is skipped.  This can speed
things up significantly and therefore can be useful in monte carlo and possibly
data mining contexts.

<br><br>
<font color=red>BUGS:</font><br>
The statistical tests performed in this function assume that an
intercept term is included in your regression model.  If no intercept term
is included, the P-values, confidence intervals and adjusted R^2 values
calculated by this function will be wrong.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>int</font>[] nBeers = [8,6,7,5,3,0,9];
<font color=blue>int</font>[] nCoffees = [3,6,2,4,3,6,8];
<font color=blue>int</font>[] musicVolume = [3,1,4,1,5,9,2];
<font color=blue>int</font>[] programmingSkill = [2,7,1,8,2,8,1];

<font color=green>// Using default confidence interval:
</font><font color=blue>auto</font> results = <u>linearRegress</u>(programmingSkill, repeat(1), nBeers, nCoffees,
    musicVolume, map!<font color=red>"a * a"</font>(musicVolume));

<font color=green>// Using user-specified confidence interval:
</font><font color=blue>auto</font> results = <u>linearRegress</u>(programmingSkill, repeat(1), nBeers, nCoffees,
    musicVolume, map!<font color=red>"a * a"</font>(musicVolume), 0.8675309);
</pre>
<br><br>

</dd>
<dt><big><a name="PolyFitRes"></a>struct <u>PolyFitRes</u>(T);
</big></dt>
<dd>Struct returned by polyFit.<br><br>

<dl><dt><big><a name="PolyFitRes.X"></a>T <u>X</u>;
</big></dt>
<dd>The array of PowMap ranges created by polyFit.<br><br>

</dd>
<dt><big><a name="PolyFitRes.regressRes"></a>RegressRes <u>regressRes</u>;
</big></dt>
<dd>The rest of the results.  This is alias this'd.<br><br>

</dd>
</dl>
</dd>
<dt><big><a name="polyFitBeta"></a>double[] <u>polyFitBeta</u>(T, U)(U <i>Y</i>, T <i>X</i>, uint <i>N</i>, double <i>ridge</i> = 0);
</big></dt>
<dd>Convenience function that takes a forward range <i>X</i> and a forward range <i>Y</i>,
 creates an array of PowMap structs for integer powers from 0 through <i>N</i>,
 and calls linearRegressBeta.
<br><br>
<b>Returns:</b><br>
An array of doubles.  The index of each element corresponds to
 the exponent.  For example, the <i>X</i><sup>2</sup> term will have an index of
 2.<br><br>

</dd>
<dt><big><a name="polyFitBetaBuf"></a>double[] <u>polyFitBetaBuf</u>(T, U)(double[] <i>buf</i>, U <i>Y</i>, T <i>X</i>, uint <i>N</i>, double <i>ridge</i> = 0);
</big></dt>
<dd>Same as polyFitBeta, but allows the caller to provide an explicit buffer
 to return the coefficients in.  If it's too short, a new one will be
 allocated.  Otherwise, results will be returned in the user-provided buffer.<br><br>

</dd>
<dt><big><a name="polyFit"></a>PolyFitRes!(PowMap!(uint, T)[]) <u>polyFit</u>(T, U)(U <i>Y</i>, T <i>X</i>, uint <i>N</i>, double <i>confInt</i> = 0.95);
</big></dt>
<dd>Convenience function that takes a forward range <i>X</i> and a forward range <i>Y</i>,
 creates an array of PowMap structs for integer powers 0 through <i>N</i>,
 and calls linearRegress.
<br><br>
<b>Returns:</b><br>
A PolyFitRes containing the array of PowMap structs created and
 a RegressRes.  The PolyFitRes is alias this'd to the RegressRes.<br><br>

</dd>
<dt><big><a name="linearRegressPenalized"></a>double[] <u>linearRegressPenalized</u>(Y, X...)(Y <i>yIn</i>, X <i>xIn</i>, double <i>lasso</i>, double <i>ridge</i>);
</big></dt>
<dd>Performs <i>lasso</i> (L1) and/or <i>ridge</i> (L2) penalized linear regression.  Due to the
way the data is standardized, no intercept term should be included in x
(unlike linearRegress and linearRegressBeta).  The intercept coefficient is
implicitly included and returned in the first element of the returned array.
Usage is otherwise identical.
<br><br>
<b>Note:</b><br>
Setting <i>lasso</i> equal to zero is equivalent to performing <i>ridge</i> regression.
       This can also be done with linearRegressBeta.  However, the
       linearRegressBeta algorithm is optimized for memory efficiency and
       large samples.  This algorithm is optimized for large feature sets.

<br><br>
<b>Returns:</b><br>
The beta coefficients for the regression model.

<br><br>
<b>References:</b><br>
Friedman J, et al Pathwise coordinate optimization. Ann. Appl. Stat.
2007;2:302-332.
<br><br>

Goeman, J. J., L1 penalized estimation in the Cox proportional hazards model.
Biometrical Journal 52(1), 70{84.
<br><br>

Eilers, P., Boer, J., Van Ommen, G., Van Houwelingen, H. 2001 Classification of
microarray data with penalized logistic regression. Proceedings of SPIE.
Progress in Biomedical Optics and Images vol. 4266, pp. 187-198
<br><br>

Douglas M. Hawkins and Xiangrong Yin.  A faster algorithm for <i>ridge</i> regression
of reduced rank data.  Computational Statistics &amp; Data Analysis Volume 40,
Issue 2, 28 August 2002, Pages 253-262

<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula<br><br>

</dd>
<dt><big><a name="logisticRegressBeta"></a>double[] <u>logisticRegressBeta</u>(T, U...)(T <i>yIn</i>, U <i>xRidge</i>);
</big></dt>
<dd>Computes a logistic regression using a maximum likelihood estimator
and returns the beta coefficients.  This is a generalized linear model with
the link function f(XB) = 1 / (1 + exp(XB)). This is generally used to model
the probability that a binary Y variable is 1 given a set of X variables.
<br><br>
For the purpose of this function, Y variables are interpreted as Booleans,
regardless of their type.  X may be either a range of ranges or a tuple of
ranges.  However, note that unlike in linearRegress, they are copied to an
array if they are not random access ranges.  Note that each value is accessed
several times, so if your range is a map to something expensive, you may
want to evaluate it eagerly.
<br><br>

If the last parameter passed in is a numeric value instead of a range,
it is interpreted as a ridge parameter and ridge regression is performed.  This
penalizes the L2 norm of the beta vector (in a scaled space) and results
in more parsimonious models.  It limits the usefulness of inference techniques
(p-values, confidence intervals), however, and is therefore not offered
in logisticRegres().
<br><br>

If no ridge parameter is passed, or equivalenty if the ridge parameter is
zero, then ordinary maximum likelihood regression is performed.
<br><br>

Note that, while this implementation of ridge regression was tested against
the R Design Package implementation, it uses slightly different conventions
that make the results not comparable without transformation.  dstats uses a
biased estimate of the variance to scale the beta vector penalties, while
Design uses an unbiased estimate.  Furthermore, Design penalizes by 1/2 of the
L2 norm, whereas dstats penalizes by the L2 norm.  Therefore, if n is the
sample size, and lambda is the penalty used with dstats, the proper penalty
to use in Design to get the same results is 2 * (n - 1) * lambda / n.
<br><br>

Also note that, as in linearRegress, repeat(1) can be used for the intercept
term.

<br><br>
<b>Returns:</b><br>
The beta coefficients for the regression model.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Logistic_regression

<br><br>
<b>http:</b><br>
//socserv.mcmaster.ca/jfox/Courses/UCLA/logistic-regression-notes.pdf
<br><br>

S. Le Cessie and J. C. Van Houwelingen.  Ridge Estimators in Logistic
Regression.  Journal of the Royal Statistical Society. Series C
(Applied Statistics), Vol. 41, No. 1(1992), pp. 191-201
<br><br>

Frank E Harrell Jr (2009). Design: Design Package. R package version 2.3-0.
<br><br>
<b>http:</b><br>
//CRAN.R-project.org/package=Design<br><br>

</dd>
<dt><big><a name="LogisticRes"></a>struct <u>LogisticRes</u>;
</big></dt>
<dd>Plain old data struct to hold the results of a logistic regression.<br><br>

<dl><dt><big><a name="LogisticRes.betas"></a>double[] <u>betas</u>;
</big></dt>
<dd>The coefficients, one for each range in X.  These will be in the order
 that the X ranges were passed in.<br><br>

</dd>
<dt><big><a name="LogisticRes.stdErr"></a>double[] <u>stdErr</u>;
</big></dt>
<dd>The standard error terms of the X ranges passed in.<br><br>

</dd>
<dt><big><a name="LogisticRes.lowerBound"></a>double[] <u>lowerBound</u>;
</big></dt>
<dd>The Wald lower confidence bounds of the beta terms, at the confidence level
    specificied.  (Default 0.95).<br><br>

</dd>
<dt><big><a name="LogisticRes.upperBound"></a>double[] <u>upperBound</u>;
</big></dt>
<dd>The Wald upper confidence bounds of the beta terms, at the confidence level
    specificied.  (Default 0.95).<br><br>

</dd>
<dt><big><a name="LogisticRes.p"></a>double[] <u>p</u>;
</big></dt>
<dd>The P-value for the alternative that the corresponding beta value is
    different from zero against the <b>null</b> that it is equal to zero.  These
    are calculated using the Wald Test.<br><br>

</dd>
<dt><big><a name="LogisticRes.nullLogLikelihood"></a>double <u>nullLogLikelihood</u>;
</big></dt>
<dd>The log likelihood for the <b>null</b> model.<br><br>

</dd>
<dt><big><a name="LogisticRes.logLikelihood"></a>double <u>logLikelihood</u>;
</big></dt>
<dd>The log likelihood for the model fit.<br><br>

</dd>
<dt><big><a name="LogisticRes.aic"></a>const pure nothrow @property @safe double <u>aic</u>();
</big></dt>
<dd>Akaike Information Criterion, which is a complexity-penalized goodness-
    of-fit score, equal to 2 * k - 2 log(L) where L is the log likelihood and
    k is the number of parameters.<br><br>

</dd>
<dt><big><a name="LogisticRes.overallP"></a>double <u>overallP</u>;
</big></dt>
<dd>The P-value for the model as a whole, based on the likelihood ratio test.
    The <b>null</b> here is that the model has no predictive value, the alternative
    is that it does have predictive value.<br><br>

</dd>
<dt><big><a name="LogisticRes.toString"></a>string <u>toString</u>();
</big></dt>
<dd>Print out the results in the default format.<br><br>

</dd>
</dl>
</dd>
<dt><big><a name="logisticRegress"></a>LogisticRes <u>logisticRegress</u>(T, V...)(T <i>yIn</i>, V <i>input</i>);
</big></dt>
<dd>Similar to logisticRegressBeta, but returns a LogisticRes with useful stuff for
statistical inference.  If the last element of <i>input</i> is a floating point
number instead of a range, it is used to specify the confidence interval
calculated.  Otherwise, the default of 0.95 is used.
<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Wald_test
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Akaike_information_criterion<br><br>

</dd>
<dt><big><a name="logistic"></a>pure nothrow @safe double <u>logistic</u>(double <i>xb</i>);
</big></dt>
<dd>The <u>logistic</u> function used in <u>logistic</u> regression.<br><br>

</dd>
<dt><big><a name="logisticRegressPenalized"></a>double[] <u>logisticRegressPenalized</u>(Y, X...)(Y <i>yIn</i>, X <i>xIn</i>, double <i>lasso</i>, double <i>ridge</i>);
</big></dt>
<dd>Performs <i>lasso</i> (L1) and/or <i>ridge</i> (L2) penalized logistic regression.  Due to the
way the data is standardized, no intercept term should be included in x
(unlike logisticRegress and logisticRegressBeta).  The intercept coefficient is
implicitly included and returned in the first element of the returned array.
Usage is otherwise identical.
<br><br>
<b>Note:</b><br>
Setting <i>lasso</i> equal to zero is equivalent to performing <i>ridge</i> regression.
       This can also be done with logisticRegressBeta.  However, the
       logisticRegressBeta algorithm is optimized for memory efficiency and
       large samples.  This algorithm is optimized for large feature sets.

<br><br>
<b>Returns:</b><br>
The beta coefficients for the regression model.

<br><br>
<b>References:</b><br>
Friedman J, et al Pathwise coordinate optimization. Ann. Appl. Stat.
2007;2:302-332.
<br><br>

Goeman, J. J., L1 penalized estimation in the Cox proportional hazards model.
Biometrical Journal 52(1), 70{84.
<br><br>

Eilers, P., Boer, J., Van Ommen, G., Van Houwelingen, H. 2001 Classification of
microarray data with penalized logistic regression. Proceedings of SPIE.
Progress in Biomedical Optics and Images vol. 4266, pp. 187-198
<br><br>

Douglas M. Hawkins and Xiangrong Yin.  A faster algorithm for <i>ridge</i> regression
of reduced rank data.  Computational Statistics &amp; Data Analysis Volume 40,
Issue 2, 28 August 2002, Pages 253-262

<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Sherman%E2%80%93Morrison_formula<br><br>

</dd>
<dt><big><a name="loess1D"></a>Loess1D <u>loess1D</u>(RX, RY)(RY <i>y</i>, RX <i>x</i>, double <i>span</i>, int <i>degree</i> = 1);
</big></dt>
<dd>This function performs loess regression.  Loess regression is a local
regression procedure, where a prediction of the dependent (<i>y</i>) variable
is made from an observation of the independent (<i>x</i>) variable by weighted
least squares over <i>x</i> values in the neighborhood of the value being evaluated.
<br><br>
In the future a separate function may be included to perform loess regression
with multiple predictors.  However, one predictor is the much more common
case and the multiple predictor case will require a much different API
and implementation, so for now only one predictor is supported.

<br><br>
<b>Params:</b><br>
<table><tr><td>RY <i>y</i></td>
<td>Observations of the dependent variable.</td></tr>
<tr><td>RX <i>x</i></td>
<td>Observations of the independent variable.</td></tr>
<tr><td>double <i>span</i></td>
<td>The fraction of <i>x</i> observations considered to be "in the neighborhood"
         when performing local regression to predict the <i>y</i> value for a new <i>x</i>.
         For example, if 8 observations are provided and <i>span</i> == 0.5,
         the 4 nearest neighbors will be used on evaluation.</td></tr>
<tr><td>int <i>degree</i></td>
<td>The polynomial <i>degree</i> of the local regression.  Must be less than
         the number of neighbors (<i>span</i> * <i>x</i>.length).</td></tr>
</table><br>
<b>Returns:</b><br>
A Loess1D object.  This object can be used to make predictions based on
the loess model.  The computations involved are done lazily, i.e. this
function sets up the Loess1D instance and returns without computing
any regression models.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>auto</font> <i>x</i> = [1, 2, 3, 4, 5, 6, 7];
<font color=blue>auto</font> <i>y</i> = [3, 6, 2, 4, 3, 6, 8];

<font color=green>// Build the Loess1D object.
</font><font color=blue>auto</font> model = <u>loess1D</u>(<i>y</i>, <i>x</i>, 0.5);

<font color=green>// Compute the weights for robust regression.
</font>model.computeRobustWeights(2);

<font color=green>// Predict the value of y when x == 5.5, using a robustness level of 2.
</font><font color=blue>auto</font> prediction = model.predict(5.5, 2);
</pre>

<br><br>
<b>References:</b><br>
Cleveland, W.S. (1979). "Robust Locally Weighted Regression and Smoothing
Scatterplots". Journal of the American Statistical Association 74 (368):
829-836.<br><br>

</dd>
<dt><big><a name="Loess1D"></a>class <u>Loess1D</u>;
</big></dt>
<dd>This class is returned from the loess1D function and holds the state of a
loess regression with one predictor variable.<br><br>

<dl><dt><big><a name="Loess1D.predict"></a>const double <u>predict</u>(double <i>point</i>, int <i>robustness</i> = 0);
</big></dt>
<dd>Predict the value of y when x == <i>point</i>, using <i>robustness</i> iterations
    of the biweight procedure outlined in the reference to make the
    estimates more robust.
<br><br>
<b>Notes:</b><br>
This function is computationally intensive but may be called
    from multiple threads simultaneously.  When predicting a
    large number of points, a parallel foreach loop may be used.
<br><br>

    Before calling this function with <i>robustness</i> &gt; 0,
    computeRobustWeights() must be called.  See this function for details.

<br><br>
<b>Returns:</b><br>
The predicted y value.<br><br>

</dd>
<dt><big><a name="Loess1D.computeRobustWeights"></a>void <u>computeRobustWeights</u>(int <i>robustness</i>, TaskPool <i>pool</i> = null);
</big></dt>
<dd>Compute the weights for robust loess, for all <i>robustness</i> levels &lt;= the
    <i>robustness</i> parameter.  This computation is embarrassingly parallel, so if a
    TaskPool is provided it will be parallelized.
<br><br>
This function must be called before calling predict() with a <i>robustness</i>
    value &gt; 0.  <u>computeRobustWeights</u>() must be called with a <i>robustness</i> level
    &gt;= the <i>robustness</i> level predict() is to be called with.  This is not
    handled implicitly because <u>computeRobustWeights</u>() is very computationally
    intensive and because it modifies the state of the Loess1D object, while
    predict() is const.  Forcing <u>computeRobustWeights</u>() to be called explicitly
    allows multiple instances of predict() to be evaluated in parallel.<br><br>

</dd>
<dt><big><a name="Loess1D.predictions"></a>const(double)[] <u>predictions</u>(int <i>robustness</i>, TaskPool <i>pool</i> = null);
</big></dt>
<dd>Obtain smoothed <u>predictions</u> of y at the values of x provided on creation
    of this object, for the given level of <i>robustness</i>.  Evaluating
    these is computationally expensive and may be parallelized by
    providing a TaskPool object.<br><br>

</dd>
<dt><big><a name="Loess1D.degree"></a>const pure nothrow @property @safe int <u>degree</u>();
</big></dt>
<dd>The polynomial <u>degree</u> for the local regression.<br><br>

</dd>
<dt><big><a name="Loess1D.x"></a>const pure nothrow @property @safe const(double)[] <u>x</u>();
</big></dt>
<dd>The <u>x</u> values provided on object creation.<br><br>

</dd>
<dt><big><a name="Loess1D.y"></a>const pure nothrow @property @safe const(double)[] <u>y</u>();
</big></dt>
<dd>The <u>y</u> values provided on object creation.<br><br>

</dd>
</dl>
</dd>
</dl>

        <hr><small>Page generated by <a href="http://dlang.org/ddoc.html">Ddoc</a>. </small>
        </body></html>
