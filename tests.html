<html><head>
        <!-- Generated by Ddoc from ../../../.dub/packages/dstats-1.0.3/dstats/source/dstats/tests.d -->
        <META http-equiv="content-type" content="text/html; charset=utf-8">
        <title>dstats.tests</title>
        </head><body>
        <h1>dstats.tests</h1>
Hypothesis testing beyond simple CDFs.  All functions work with input
 ranges with elements implicitly convertible to double unless otherwise noted.
<br><br>
<b>Author:</b><br>
David Simcha<br><br>

<dl><dt><big><a name="Alt"></a>enum <u>Alt</u>: int;
</big></dt>
<dd>Alternative hypotheses.  Exact meaning varies with test used.<br><br>

<dl><dt><big><a name="Alt.twoSided"></a><u>twoSided</u></big></dt>
<dd>f(input1) != X<br><br>

</dd>
<dt><big><a name="Alt.less"></a><u>less</u></big></dt>
<dd>f(input1) &lt; X<br><br>

</dd>
<dt><big><a name="Alt.greater"></a><u>greater</u></big></dt>
<dd>f(input1) &gt; X<br><br>

</dd>
<dt><big><a name="Alt.none"></a><u>none</u></big></dt>
<dd>Skip P-value computation (and confidence intervals if applicable)
    and just return the test statistic.<br><br>

</dd>
</dl>
</dd>
<dt><big><a name="TestRes"></a>struct <u>TestRes</u>;
</big></dt>
<dd>A plain old data struct for returning the results of hypothesis tests.<br><br>

<dl><dt><big><a name="TestRes.testStat"></a>double <u>testStat</u>;
</big></dt>
<dd>The test statistic.  What exactly this is is specific to the test.<br><br>

</dd>
<dt><big><a name="TestRes.p"></a>double <u>p</u>;
</big></dt>
<dd>The P-value against the provided alternative.  This struct can
 be implicitly converted to just the P-value via alias this.<br><br>

</dd>
<dt><big><a name="TestRes.toString"></a>string <u>toString</u>();
</big></dt>
<dd><br><br>
</dd>
</dl>
</dd>
<dt><big><a name="ConfInt"></a>struct <u>ConfInt</u>;
</big></dt>
<dd>A plain old data struct for returning the results of hypothesis tests
that also produce confidence intervals.  Contains, can implicitly convert
to, a TestRes.<br><br>

<dl><dt><big><a name="ConfInt.testRes"></a>TestRes <u>testRes</u>;
</big></dt>
<dd>This is alias this'd.<br><br>

</dd>
<dt><big><a name="ConfInt.lowerBound"></a>double <u>lowerBound</u>;
</big></dt>
<dd>Lower bound of the confidence interval at the level specified.<br><br>

</dd>
<dt><big><a name="ConfInt.upperBound"></a>double <u>upperBound</u>;
</big></dt>
<dd>Upper bound of the confidence interval at the level specified.<br><br>

</dd>
<dt><big><a name="ConfInt.toString"></a>string <u>toString</u>();
</big></dt>
<dd><br><br>
</dd>
</dl>
</dd>
<dt><big><a name="isSummary"></a>enum bool <u>isSummary</u>(T);
</big></dt>
<dd>Tests whether a struct/class has the necessary information for calculating
a T-test.  It must have a property .mean (mean), .stdev (stdandard deviation),
.var (variance), and .N (sample size).<br><br>

</dd>
<dt><big><a name="studentsTTest"></a>ConfInt <u>studentsTTest</u>(T)(T <i>data</i>, double <i>testMean</i> = 0, Alt <i>alt</i> = Alt.twoSided, double <i>confLevel</i> = 0.95) if (isSummary!T || doubleIterable!T);
</big></dt>
<dd>One-sample Student's T-test for difference between mean of <i>data</i> and
 a fixed value.  Alternatives are Alt.less, meaning mean(<i>data</i>) &lt; <i>testMean</i>,
 Alt.greater, meaning mean(<i>data</i>) &gt; <i>testMean</i>, and Alt.twoSided, meaning
 mean(<i>data</i>)!= <i>testMean</i>.
<br><br>
<i>data</i> may be either an iterable with elements implicitly convertible to
 double or a summary struct (see isSummary).

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>uint</font>[] <i>data</i> = [1,2,3,4,5];

<font color=green>// Test the null hypothesis that the mean of data is &gt;= 1 against the
</font><font color=green>// alternative that the mean of data is &lt; 1.  Calculate confidence
</font><font color=green>// intervals at 90%.
</font><font color=blue>auto</font> result1 = <u>studentsTTest</u>(<i>data</i>, 1, Alt.less, 0.9);

<font color=green>// Do the same thing, only this time we've already calculated the summary
</font><font color=green>// statistics explicitly before passing them to studensTTest.
</font><font color=blue>auto</font> summary = meanStdev(<i>data</i>);
writeln(summary.stdev);
result2 = <u>studentsTTest</u>(summary, 1, Alt.less, 0.9);  <font color=green>// Same as result1.
</font><font color=blue>assert</font>(result1 == result2);
</pre>

<br><br>
<b>Returns:</b><br>
A ConfInt containing T, the P-value and the boundaries of
 the confidence interval for mean(<i>data</i>) at the level specified.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Student%27s_t-test<br><br>

</dd>
<dt><big><a name="studentsTTest.2"></a>ConfInt <u>studentsTTest</u>(T, U)(T <i>sample1</i>, U <i>sample2</i>, double <i>testMean</i> = 0, Alt <i>alt</i> = Alt.twoSided, double <i>confLevel</i> = 0.95) if ((doubleIterable!T || isSummary!T) &amp;&amp; (doubleIterable!U || isSummary!U));
</big></dt>
<dd>Two-sample T test for a difference in means,
assumes variances of samples are equal.  Alteratives are Alt.less, meaning
mean(<i>sample1</i>) - mean(<i>sample2</i>) &lt; <i>testMean</i>, Alt.greater, meaning
mean(<i>sample1</i>) - mean(<i>sample2</i>) &gt; <i>testMean</i>, and Alt.twoSided, meaning
mean(<i>sample1</i>) - mean(<i>sample2</i>) != <i>testMean</i>.
<br><br>
<i>sample1</i> and <i>sample2</i> may be either iterables with elements implicitly
convertible to double or summary structs (see isSummary).

<br><br>
<b>Returns:</b><br>
A ConfInt containing the T statistic, the P-value, and the
boundaries of the confidence interval for the difference between means
of <i>sample1</i> and <i>sample2</i> at the specified level.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Student%27s_t-test<br><br>

</dd>
<dt><big><a name="welchTTest"></a>ConfInt <u>welchTTest</u>(T, U)(T <i>sample1</i>, U <i>sample2</i>, double <i>testMean</i> = 0, Alt <i>alt</i> = Alt.twoSided, double <i>confLevel</i> = 0.95) if ((isSummary!T || doubleIterable!T) &amp;&amp; (isSummary!U || doubleIterable!U));
</big></dt>
<dd>Two-sample T-test for difference in means.  Does not assume variances are equal.
Alteratives are Alt.less, meaning mean(<i>sample1</i>) - mean(<i>sample2</i>) &lt; <i>testMean</i>,
Alt.greater, meaning mean(<i>sample1</i>) - mean(<i>sample2</i>) &gt; <i>testMean</i>, and
Alt.twoSided, meaning mean(<i>sample1</i>) - mean(<i>sample2</i>) != <i>testMean</i>.
<br><br>
<i>sample1</i> and <i>sample2</i> may be either iterables with elements implicitly
convertible to double or summary structs (see isSummary).

<br><br>
<b>Returns:</b><br>
A ConfInt containing the T statistic, the P-value, and the
boundaries of the confidence interval for the difference between means
of <i>sample1</i> and <i>sample2</i> at the specified level.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Student%27s_t-test<br><br>

</dd>
<dt><big><a name="pairedTTest"></a>ConfInt <u>pairedTTest</u>(T, U)(T <i>before</i>, U <i>after</i>, double <i>testMean</i> = 0, Alt <i>alt</i> = Alt.twoSided, double <i>confLevel</i> = 0.95) if (doubleInput!T &amp;&amp; doubleInput!U &amp;&amp; isInputRange!T &amp;&amp; isInputRange!U);
</big></dt>
<dd>Paired T test.  Tests the hypothesis that the mean difference between
corresponding elements of <i>before</i> and <i>after</i> is <i>testMean</i>.  Alternatives are
Alt.less, meaning the that the <b>true</b> mean difference (<i>before</i>[i] - <i>after</i>[i])
is less than <i>testMean</i>, Alt.greater, meaning the <b>true</b> mean difference is
greater than <i>testMean</i>, and Alt.twoSided, meaning the <b>true</b> mean difference is not
equal to <i>testMean</i>.
<br><br>
<i>before</i> and <i>after</i> must be input ranges with elements implicitly convertible
to double and must have the same length.

<br><br>
<b>Returns:</b><br>
A ConfInt containing the T statistic, the P-value, and the
boundaries of the confidence interval for the mean difference between
corresponding elements of sample1 and sample2 at the specified level.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Student%27s_t-test<br><br>

</dd>
<dt><big><a name="pairedTTest.2"></a>ConfInt <u>pairedTTest</u>(T)(T <i>diffSummary</i>, double <i>testMean</i> = 0, Alt <i>alt</i> = Alt.twoSided, double <i>confLevel</i> = 0.95) if (isSummary!T);
</big></dt>
<dd>Compute a paired T test directly from summary statistics of the differences
between corresponding samples.
<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>float</font>[] data1 = [8, 6, 7, 5, 3, 0, 9];
<font color=blue>float</font>[] data2 = [3, 6, 2, 4, 3, 6, 8];

<font color=green>// Calculate summary statistics on difference explicitly.
</font>MeanSD summary;
<font color=blue>foreach</font>(i; 0..data1.length) {
    summary.put(data1[i] - data2[i]);
}

<font color=green>// Test the null hypothesis that the mean difference between corresponding
</font><font color=green>// elements (data1[i] - data2[i]) is greater than 5 against the null that it
</font><font color=green>// is &lt;= 5.  Calculate confidence intervals at 99%.
</font><font color=blue>auto</font> result = <u>pairedTTest</u>(summary, 5, Alt.twoSided, 0.99);

<font color=green>// This is equivalent to:
</font><font color=blue>auto</font> result2 = <u>pairedTTest</u>(data1, data2, 5, Alt.twoSided, 0.99);
</pre>

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Student%27s_t-test<br><br>

</dd>
<dt><big><a name="levenesTest"></a>TestRes <u>levenesTest</u>(alias central = median, T...)(T <i>data</i>);
</big></dt>
<dd>Tests the <b>null</b> hypothesis that the variances of all groups are equal against
the alternative that heteroscedasticity exists.  <i>data</i> must be either a
tuple of ranges or a range of ranges.  central is an alias for the measure
of central tendency to be used.  This can be any function that maps a
forward range of numeric types to a numeric type.  The commonly used ones
are median (default) and mean (less robust).  Trimmed mean is sometimes
useful, but is currently not implemented in dstats.summary.
<br><br>
<b>References:</b><br>
Levene, Howard (1960). "Robust tests for equality of variances". in Ingram
Olkin, Harold Hotelling et al. Contributions to Probability and Statistics:
Essays in Honor of Harold Hotelling. Stanford University Press. pp. 278-292.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>int</font>[] sample1 = [1,2,3,4,5];
<font color=blue>int</font>[] sample2 = [100,200,300,400,500];
<font color=blue>auto</font> result = <u>levenesTest</u>(sample1, sample2);

<font color=green>// Clearly the variances are different between these two samples.
</font><font color=blue>assert</font>( approxEqual(result.testStat, 10.08));
<font color=blue>assert</font>( approxEqual(result.p, 0.01310));
</pre>
<br><br>

</dd>
<dt><big><a name="fTest"></a>TestRes <u>fTest</u>(T...)(T <i>data</i>);
</big></dt>
<dd>The F-test is a one-way ANOVA extension of the T-test to &gt;2 groups.
It's useful when you have 3 or more groups with equal variance and want
to test whether their means are equal.  Data can be input as either a
tuple or a range.  This may contain any combination of ranges of numeric
types, MeanSD structs and Summary structs.
<br><br>
<b>Note:</b><br>
This test makes the assumption that all groups have equal variances,
also known as homoskedasticity.  For a similar test that does not make these
assumptions, see welchAnova.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>uint</font>[] thing1 = [3,1,4,1],
       thing2 = [5,9,2,6,5,3],
       thing3 = [5,8,9,7,9,3];
<font color=blue>auto</font> result = <u>fTest</u>(thing1, meanStdev(thing2), summary(thing3));
<font color=blue>assert</font>(approxEqual(result.testStat, 4.9968));
<font color=blue>assert</font>(approxEqual(result.p, 0.02456));
</pre>

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/F-test

<br><br>
<b>Returns:</b><br>
A TestRes containing the F statistic and the P-value for the alternative
that the means of the groups are different against the <b>null</b> that they
are identical.<br><br>

</dd>
<dt><big><a name="welchAnova"></a>TestRes <u>welchAnova</u>(T...)(T <i>data</i>);
</big></dt>
<dd>Same as fTest, except that this test does not require the assumption of
equal variances.  In exchange it's slightly less powerful.
<br><br>
<b>References:</b><br>
B.L. Welch. On the Comparison of Several Mean Values: An Alternative Approach
Biometrika, Vol. 38, No. 3/4 (Dec., 1951), pp. 330-336.<br><br>

</dd>
<dt><big><a name="correlatedAnova"></a>TestRes <u>correlatedAnova</u>(T...)(T <i>dataIn</i>) if (allSatisfy!(isInputRange, T));
</big></dt>
<dd>Performs a correlated sample (within-subjects) ANOVA.  This is a
 generalization of the paired T-test to 3 or more treatments.  This
 function accepts data as either a tuple of ranges (1 for each treatment,
 such that a given index represents the same subject in each range) or
 similarly as a range of ranges.
<br><br>
<b>Returns:</b><br>
A TestRes with the F-statistic and P-value for the <b>null</b> that
 the the variable being measured did not vary across treatments against the
 alternative that it did.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=green>// Test the hypothesis that alcohol, loud music, caffeine and sleep
</font><font color=green>// deprivation all have equivalent effects on programming ability.
</font>
<font color=blue>uint</font>[] alcohol = [8,6,7,5,3,0,9];
<font color=blue>uint</font>[] caffeine = [3,6,2,4,3,6,8];
<font color=blue>uint</font>[] noSleep = [3,1,4,1,5,9,2];
<font color=blue>uint</font>[] loudMusic = [2,7,1,8,2,8,1];
<font color=green>// Subject 0 had ability of 8 under alcohol, 3 under caffeine, 3 under
</font><font color=green>// no sleep, 2 under loud music.  Subject 1 had ability of 6 under alcohol,
</font><font color=green>// 6 under caffeine, 1 under no sleep, and 7 under loud music, etc.
</font><font color=blue>auto</font> result = <u>correlatedAnova</u>(alcohol, caffeine, noSleep, loudMusic);
</pre>

<br><br>
<b>References:</b><br>
"Concepts and Applications of Inferrential Statistics".
              Richard Lowry.  Vassar College.   version.
<br><br>
<b>http:</b><br>
//faculty.vassar.edu/lowry/webtext.html<br><br>

</dd>
<dt><big><a name="kruskalWallis"></a>TestRes <u>kruskalWallis</u>(T...)(T <i>dataIn</i>) if (doubleInput!(typeof(<i>dataIn</i>[0].front)) || allSatisfy!(doubleInput, T));
</big></dt>
<dd>The Kruskal-Wallis rank sum test.  Tests the <b>null</b> hypothesis that data in
 each group is not stochastically ordered with respect to data in each other
 groups.  This is a one-way non-parametric ANOVA and can be thought of
 as either a generalization of the Wilcoxon rank sum test to &gt;2 groups or
 a non-parametric equivalent to the F-test.  Data can be input as either a
 tuple of ranges (one range for each group) or a range of ranges
 (one element for each group).
<br><br>
<font color=red>BUGS:</font><br>
Asymptotic approximation of P-value only, not exact.  In this case,
 I'm not sure a practical way to compute the exact P-value even exists.

<br><br>
<b>Returns:</b><br>
A TestRes with the K statistic and the P-value for the <b>null</b> that
 no group is stochastically larger than any other against the alternative that
 groups are stochastically ordered.

<br><br>
<b>References:</b><br>
"Concepts and Applications of Inferrential Statistics".
              Richard Lowry.  Vassar College.   version.
<br><br>
<b>http:</b><br>
//faculty.vassar.edu/lowry/webtext.html<br><br>

</dd>
<dt><big><a name="friedmanTest"></a>TestRes <u>friedmanTest</u>(T...)(T <i>dataIn</i>) if (doubleInput!(typeof(<i>dataIn</i>[0].front)) || allSatisfy!(doubleInput, T));
</big></dt>
<dd>The Friedman test is a non-parametric within-subject ANOVA.  It's useful
 when parametric assumptions cannot be made.  Usage is identical to
 correlatedAnova().
<br><br>
<b>References:</b><br>
"Concepts and Applications of Inferrential Statistics".
              Richard Lowry.  Vassar College.   version.
<br><br>
<b>http:</b><br>
//faculty.vassar.edu/lowry/webtext.html

<br><br>
<font color=red>BUGS:</font><br>
No exact P-value calculation.  Asymptotic approx. only.<br><br>

</dd>
<dt><big><a name="wilcoxonRankSum"></a>TestRes <u>wilcoxonRankSum</u>(T, U)(T <i>sample1</i>, U <i>sample2</i>, Alt <i>alt</i> = Alt.twoSided, uint <i>exactThresh</i> = 50) if (isInputRange!T &amp;&amp; isInputRange!U &amp;&amp; is(typeof(<i>sample1</i>.front &lt; <i>sample2</i>.front) == bool) &amp;&amp; is(CommonType!(ElementType!T, ElementType!U)));
</big></dt>
<dd>Computes Wilcoxon rank sum test statistic and P-value for
 a set of observations against another set, using the given alternative.
 Alt.less means that <i>sample1</i> is stochastically less than <i>sample2</i>.
 Alt.greater means <i>sample1</i> is stochastically greater than <i>sample2</i>.
 Alt.twoSided means <i>sample1</i> is stochastically less than or greater than
 <i>sample2</i>.
<br><br>
<i>exactThresh</i> is the threshold value of (n1 + n2) at which this function
 switches from exact to approximate computation of the p-value.  Do not set
 <i>exactThresh</i> to more than 200, as the exact
 calculation is both very slow and not numerically stable past this point,
 and the asymptotic calculation is very good for N this large.  To disable
 exact calculation entirely, set <i>exactThresh</i> to 0.

<br><br>
<b>Notes:</b><br>
Exact p-value computation is never used when ties are present in the
 data, because it is not computationally feasible.
<br><br>

 Input ranges for this function must define a length.
<br><br>

 This test is also known as the Mann-Whitney U test.

<br><br>
<b>Returns:</b><br>
A TestRes containing the W test statistic and the P-value against
 the given alternative.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U
<br><br>

 StackOverflow Question 376003  http://stackoverflow.com/questions/376003
<br><br>

 Loughborough University MLSC Statistics 2.3 The Mann-Whitney U Test
<br><br>
<b>http:</b><br>
//mlsc.lboro.ac.uk/resources/statistics/Mannwhitney.pdf<br><br>

</dd>
<dt><big><a name="wilcoxonSignedRank"></a>TestRes <u>wilcoxonSignedRank</u>(T, U)(T <i>before</i>, U <i>after</i>, Alt <i>alt</i> = Alt.twoSided, uint <i>exactThresh</i> = 50) if (doubleInput!T &amp;&amp; doubleInput!U &amp;&amp; is(typeof(<i>before</i>.front - <i>after</i>.front) : double));
</big></dt>
<dd>Computes a test statistic and P-value for a Wilcoxon signed rank test against
 the given alternative. Alt.less means that elements of <i>before</i> are stochastically
 less than corresponding elements of <i>after</i>.  Alt.greater means elements of
 <i>before</i> are stochastically greater than corresponding elements of <i>after</i>.
 Alt.twoSided means there is a significant difference in either direction.
<br><br>
<i>exactThresh</i> is the threshold value of <i>before</i>.length at which this function
 switches from exact to approximate computation of the p-value.   Do not set
 <i>exactThresh</i> to more than 200, as the exact calculation is both very slow and
 not numerically stable past this point, and the asymptotic calculation is
 very good for N this large.  To disable exact calculation entirely, set
 <i>exactThresh</i> to 0.

<br><br>
<b>Notes:</b><br>
Exact p-value computation is never used when ties are present,
 because it is not computationally feasible.
<br><br>

 The input ranges for this function must define a length and must be
 forward ranges.

<br><br>
<b>Returns:</b><br>
A TestRes of the W statistic and the p-value against the given
 alternative.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
<br><br>

 StackOverflow Question 376003  http://stackoverflow.com/questions/376003
<br><br>

 Handbook of Parametric and nonparametric statistical procedures. David Sheskin.
 Third Edition. (2004)  CRC Press. Pg. 616.<br><br>

</dd>
<dt><big><a name="wilcoxonSignedRank.2"></a>TestRes <u>wilcoxonSignedRank</u>(T)(T <i>data</i>, double <i>mu</i>, Alt <i>alt</i> = Alt.twoSided, uint <i>exactThresh</i> = 50) if (doubleInput!T &amp;&amp; is(typeof(<i>data</i>.front - <i>mu</i>) : double));
</big></dt>
<dd>Same as the overload, but allows testing whether a range is stochastically
 less than or greater than a fixed value <i>mu</i> rather than paired elements of
 a second range.<br><br>

</dd>
<dt><big><a name="signTest"></a>TestRes <u>signTest</u>(T, U)(T <i>before</i>, U <i>after</i>, Alt <i>alt</i> = Alt.twoSided) if (doubleInput!T &amp;&amp; doubleInput!U &amp;&amp; is(typeof(<i>before</i>.front &lt; <i>after</i>.front) == bool));
</big></dt>
<dd>Sign test for differences between paired values.  This is a very robust
 but very low power test.  Alternatives are Alt.less, meaning elements
 of <i>before</i> are typically less than corresponding elements of <i>after</i>,
 Alt.greater, meaning elements of <i>before</i> are typically greater than
 elements of <i>after</i>, and Alt.twoSided, meaning that there is a significant
 difference in either direction.
<br><br>
<b>Returns:</b><br>
A TestRes with the proportion of elements of <i>before</i> that were
 greater than the corresponding element of <i>after</i>, and the P-value against
 the given alternative.<br><br>

</dd>
<dt><big><a name="signTest.2"></a>TestRes <u>signTest</u>(T)(T <i>data</i>, double <i>mu</i>, Alt <i>alt</i> = Alt.twoSided) if (doubleInput!T &amp;&amp; is(typeof(<i>data</i>.front &lt; <i>mu</i>) == bool));
</big></dt>
<dd>Similar to the overload, but allows testing for a difference between a
 range and a fixed value <i>mu</i>.<br><br>

</dd>
<dt><big><a name="binomialTest"></a>double <u>binomialTest</u>(ulong <i>k</i>, ulong <i>n</i>, double <i>p</i>);
</big></dt>
<dd>Two-sided binomial test for whether P(success) == <i>p</i>.  The one-sided
alternatives are covered by dstats.distrib.binomialCDF and binomialCDFR.
<i>k</i> is the number of successes observed, <i>n</i> is the number of trials, <i>p</i>
is the probability of success under the <b>null</b>.
<br><br>
<b>Returns:</b><br>
The P-value for the alternative that P(success) != <i>p</i> against
the <b>null</b> that P(success) == <i>p</i>.

<br><br>
<b>Notes:</b><br>
This test can also be performed using multinomialTest, but this
implementation is much faster and easier to use.<br><br>

</dd>
<dt><big><a name="Expected"></a>enum <u>Expected</u>: int;
</big></dt>
<dd>For chiSquareFit and gTestFit, is expected value range counts or proportions?<br><br>

<dl><dt><big><a name="Expected.count"></a><u>count</u></big></dt>
<dd><br><br>
</dd>
<dt><big><a name="Expected.proportion"></a><u>proportion</u></big></dt>
<dd><br><br>
</dd>
</dl>
</dd>
<dt><big><a name="chiSquareFit"></a>TestRes <u>chiSquareFit</u>(T, U)(T <i>observed</i>, U <i>expected</i>, Expected <i>countProp</i> = Expected.proportion) if (doubleInput!T &amp;&amp; doubleInput!U);
</big></dt>
<dd>Performs a one-way Pearson's chi-square goodness of fit test between a range
of <i>observed</i> and a range of <i>expected</i> values.  This is a useful statistical
test for testing whether a set of observations fits a discrete distribution.
<br><br>
<b>Returns:</b><br>
A TestRes of the chi-square statistic and the P-value for the
alternative hypothesis that <i>observed</i> is not a sample from <i>expected</i> against
the <b>null</b> that <i>observed</i> is a sample from <i>expected</i>.

<br><br>
<b>Notes:</b><br>
By default, <i>expected</i> is assumed to be a range of <i>expected</i> proportions.
These proportions are automatically normalized, and can sum to any number.
By passing Expected.count in as the last parameter, calculating <i>expected</i>
counts will be skipped, and <i>expected</i> will assume to already be properly
normalized.  This is slightly faster, but more importantly
allows input ranges to be used.
<br><br>

The chi-square test relies on asymptotic statistical properties
and is therefore not considered valid, as a rule of thumb,  when <i>expected</i>
counts are below 5.  However, this rule is likely to be unnecessarily
stringent in most cases.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=green>// Test to see whether a set of categorical observations differs
</font><font color=green>// statistically from a discrete uniform distribution.
</font>
<font color=blue>uint</font>[] <i>observed</i> = [980, 1028, 1001, 964, 1102];
<font color=blue>auto</font> <i>expected</i> = repeat(1.0);
<font color=blue>auto</font> res2 = <u>chiSquareFit</u>(<i>observed</i>, <i>expected</i>);
<font color=blue>assert</font>(approxEqual(res2, 0.0207));
<font color=blue>assert</font>(approxEqual(res2.testStat, 11.59));
</pre>

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Pearson%27s_chi-square_test<br><br>

</dd>
<dt><big><a name="gTestFit"></a>TestRes <u>gTestFit</u>(T, U)(T <i>observed</i>, U <i>expected</i>, Expected <i>countProp</i> = Expected.proportion) if (doubleInput!T &amp;&amp; doubleInput!U);
</big></dt>
<dd>The G or likelihood ratio chi-square test for goodness of fit.  Roughly
 the same as Pearson's chi-square test (chiSquareFit), but may be more
 accurate in certain situations and less accurate in others.  However, it is
 still based on asymptotic distributions, and is not exact. Usage is is
 identical to chiSquareFit.
<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/G_test<br><br>

</dd>
<dt><big><a name="multinomialTest"></a>double <u>multinomialTest</u>(U, F)(U <i>countsIn</i>, F <i>proportions</i>) if (isInputRange!U &amp;&amp; isInputRange!F &amp;&amp; isIntegral!(ElementType!U) &amp;&amp; isFloatingPoint!(ElementType!F));
</big></dt>
<dd>The exact multinomial goodness of fit test for whether a set of counts
fits a hypothetical distribution.  counts is an input range of counts.
<i>proportions</i> is an input range of expected <i>proportions</i>.  These are normalized
automatically, so they can sum to any value.
<br><br>
<b>Returns:</b><br>
The P-value for the <b>null</b> that counts is a sample from <i>proportions</i>
against the alternative that it isn't.

<br><br>
<b>Notes:</b><br>
This test is EXTREMELY slow for anything but very small samples and
degrees of freedom.  The Pearson's chi-square (chiSquareFit()) or likelihood
ratio chi-square (gTestFit()) are good enough approximations unless sample
size is very small.<br><br>

</dd>
<dt><big><a name="chiSquareContingency"></a>TestRes <u>chiSquareContingency</u>(T...)(T <i>inputData</i>);
</big></dt>
<dd>Performs a Pearson's chi-square test on a contingency table of arbitrary
dimensions.  When the chi-square test is mentioned, this is usually the one
being referred to.  Takes a set of finite forward ranges, one for each column
in the contingency table.  These can be expressed either as a tuple of ranges
or a range of ranges.  Returns a P-value for the alternative hypothesis that
frequencies in each row of the contingency table depend on the column against
the <b>null</b> that they don't.
<br><br>
<b>Notes:</b><br>
The chi-square test relies on asymptotic statistical properties
and is therefore not exact.  The typical rule of thumb is that each cell
should have an expected value of at least 5.  However, this is likely to
be unnecessarily stringent.
<br><br>

Yates's continuity correction is never used in this implementation.  If
you want something that's guaranteed to be conservative, use fisherExact().
<br><br>

This is, for all practical purposes, an inherently non-directional test.
Therefore, the one-sided verses two-sided option is not provided.
<br><br>

For 2x2 contingency tables, fisherExact is a more conservative test, in that
the type I error rate is guaranteed to never be above the nominal P-value.
However, even for small sample sizes this test may produce results closer
to the <b>true</b> P-value, at the risk of possibly being non-conservative.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=green>// Test to see whether the relative frequency of outcome 0, 1, and 2
</font><font color=green>// depends on the treatment (drug1, drug2 or placebo) in some hypothetical
</font><font color=green>// experiment.  For example, 1500 people had outcome 2 if they were treated
</font><font color=green>// with drug1 and 1100 had outcome 1 if treated with placebo.
</font><font color=blue>uint</font>[] drug1 = [1000, 2000, 1500];
<font color=blue>uint</font>[] drug2 = [1500, 3000, 2300];
<font color=blue>uint</font>[] placebo = [500, 1100, 750];
<font color=blue>auto</font> result1 = <u>chiSquareContingency</u>(drug1, drug2, placebo);

<font color=green>// The following uses a range of ranges instead of an array of ranges,
</font><font color=green>// and will produce equivalent results.
</font><font color=blue>auto</font> rangeOfRanges = [drug1, drug2, placebo];
<font color=blue>auto</font> result2 = <u>chiSquareContingency</u>(rangeOfRanges);
</pre>

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Pearson%27s_chi-square_test<br><br>

</dd>
<dt><big><a name="GTestRes"></a>struct <u>GTestRes</u>;
</big></dt>
<dd>This struct is a subtype of TestRes and is used to return the results of
gTestContingency and gTestObs.  Due to the information theoretic interpretation
of the G test, it contains an extra field to return the mutual information
in bits.<br><br>

<dl><dt><big><a name="GTestRes.testRes"></a>TestRes <u>testRes</u>;
</big></dt>
<dd><br><br>
</dd>
<dt><big><a name="GTestRes.mutualInfo"></a>double <u>mutualInfo</u>;
</big></dt>
<dd>The mutual info of the two random variables in the joint distribution
    represented by the contingency table, in bits (base 2).<br><br>

</dd>
</dl>
</dd>
<dt><big><a name="gTestContingency"></a>GTestRes <u>gTestContingency</u>(T...)(T <i>inputData</i>);
</big></dt>
<dd>The G or likelihood ratio chi-square test for contingency tables.  Roughly
the same as Pearson's chi-square test (chiSquareContingency), but may be more
accurate in certain situations and less accurate in others.
<br><br>
Like Pearson's Chi-square, the G-test is based on asymptotic distributions,
and is not exact. Usage is is identical to chiSquareContingency.

<br><br>
<b>Note:</b><br>
This test can be thought of as a test for nonzero mutual information
between the random variables represented by the rows and the columns,
since the test statistic and P-value are strictly increasing
and strictly decreasing, respectively, in mutual information.  Therefore, this
function returns a GTestRes, which is a subtype of TestRes and also gives
the mutual information for use in information theoretic settings.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/G_test, last retrieved 1/22/2011<br><br>

</dd>
<dt><big><a name="chiSquareObs"></a>TestRes <u>chiSquareObs</u>(T, U)(T <i>x</i>, U <i>y</i>) if (isInputRange!T &amp;&amp; isInputRange!U);
</big></dt>
<dd>Given two vectors of observations of jointly distributed variables <i>x</i>, <i>y</i>, tests
the <b>null</b> hypothesis that values in <i>x</i> are independent of the corresponding
values in <i>y</i>.  This is done using Pearson's Chi-Square Test.  For a similar test
that assumes the data has already been tabulated into a contingency table, see
chiSquareContingency.
<br><br>
<i>x</i> and <i>y</i> must both be input ranges.  If they are not the same length, a
DstatsArgumentException is thrown.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=green>// Test whether the appearance of "foo" vs. "bar" is independent of the
</font><font color=green>// appearance of "baz" vs. "xxx".
</font><font color=blue>auto</font> <i>x</i> = [<font color=red>"foo"</font>, <font color=red>"bar"</font>, <font color=red>"bar"</font>, <font color=red>"foo"</font>, <font color=red>"foo"</font>];
<font color=blue>auto</font> <i>y</i> = [<font color=red>"xxx"</font>, <font color=red>"baz"</font>, <font color=red>"baz"</font>, <font color=red>"xxx"</font>, <font color=red>"baz"</font>];
<font color=blue>auto</font> result = <u>chiSquareObs</u>(<i>x</i>, <i>y</i>);

<font color=green>// This is equivalent to:
</font><font color=blue>auto</font> contingency = <font color=blue>new</font> <font color=blue>uint</font>[][](2, 2);
<font color=blue>foreach</font>(i; 0..<i>x</i>.length) {
    <font color=blue>immutable</font> index1 = (<i>x</i>[i] == <font color=red>"foo"</font>);
    <font color=blue>immutable</font> index2 = (<i>y</i>[i] == <font color=red>"xxx"</font>);
    contingency[index1][index2]++;
}

<font color=blue>auto</font> result2 = chiSquareContingency(contingency);
</pre>
<br><br>

</dd>
<dt><big><a name="gTestObs"></a>GTestRes <u>gTestObs</u>(T, U)(T <i>x</i>, U <i>y</i>) if (isInputRange!T &amp;&amp; isInputRange!U);
</big></dt>
<dd>Given two ranges of observations of jointly distributed variables <i>x</i>, <i>y</i>, tests
the <b>null</b> hypothesis that values in <i>x</i> are independent of the corresponding
values in <i>y</i>.  This is done using the Likelihood Ratio G test.  Usage is similar
to chiSquareObs.  For an otherwise identical test that assumes the data has
already been tabulated into a contingency table, see gTestContingency.
<br><br>
<b>Note:</b><br>
This test can be thought of as a test for nonzero mutual information
between <i>x</i> and <i>y</i>, since the test statistic and P-value are strictly increasing
and strictly decreasing, respectively, in mutual information.  Therefore, this
function returns a GTestRes, which is a subtype of TestRes and also gives
the mutual information for use in information theoretic settings.<br><br>

</dd>
<dt><big><a name="fisherExact"></a>TestRes <u>fisherExact</u>(T)(const T[2][2] <i>contingencyTable</i>, Alt <i>alt</i> = Alt.twoSided) if (isIntegral!T);
</big></dt>
<dd>Fisher's Exact test for difference in odds between rows/columns
in a 2x2 contingency table.  Specifically, this function tests the odds
ratio, which is defined, for a contingency table c, as (c[0][0] * c[1][1])
 / (c[1][0] * c[0][1]).  Alternatives are Alt.less, meaning <b>true</b> odds ratio
&lt; 1, Alt.greater, meaning <b>true</b> odds ratio &gt; 1, and Alt.twoSided, meaning
<b>true</b> odds ratio != 1.
<br><br>
Accepts a 2x2 contingency table as an array of arrays of uints.
For now, only does 2x2 contingency tables.

<br><br>
<b>Notes:</b><br>
Although this test is "exact" in that it does not rely on asymptotic
approximations, it is very statistically conservative when the marginals
are not truly fixed in the experimental design in question.  If a
closer but possibly non-conservative approximation of the <b>true</b> P-value is
desired, Pearson's chi-square test (chiSquareContingency) may perform better,
even for small samples.

<br><br>
<b>Returns:</b><br>
A TestRes of the odds ratio and the P-value against the given
alternative.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>double</font> res = <u>fisherExact</u>([[2u, 7], [8, 2]], Alt.less);
<font color=blue>assert</font>(approxEqual(res.p, 0.01852));  <font color=green>// Odds ratio is very small in this case.
</font><font color=blue>assert</font>(approxEqual(res.testStat, 4.0 / 56.0));
</pre>

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Fisher%27s_Exact_Test<br><br>

</dd>
<dt><big><a name="fisherExact.2"></a>TestRes <u>fisherExact</u>(T)(const T[][] <i>contingencyTable</i>, Alt <i>alt</i> = Alt.twoSided) if (isIntegral!T);
</big></dt>
<dd>Convenience function.  Converts a dynamic array to a static one, then
calls the overload.<br><br>

</dd>
<dt><big><a name="ksTest"></a>TestRes <u>ksTest</u>(T, U)(T <i>F</i>, U <i>Fprime</i>) if (doubleInput!T &amp;&amp; doubleInput!U);
</big></dt>
<dd>Performs a Kolmogorov-Smirnov (K-S) 2-sample test.  The K-S test is a
non-parametric test for a difference between two empirical distributions or
between an empirical distribution and a reference distribution.
<br><br>
<b>Returns:</b><br>
A TestRes with the K-S D value and a P value for the <b>null</b> that
FPrime is distributed identically to <i>F</i> against the alternative that it isn't.
This implementation uses a signed D value to indicate the direction of the
difference between distributions.  To get the D value used in standard
notation, simply take the absolute value of this D value.

<br><br>
<font color=red>BUGS:</font><br>
Exact calculation not implemented.  Uses asymptotic approximation.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test<br><br>

</dd>
<dt><big><a name="ksTest.2"></a>TestRes <u>ksTest</u>(T, Func)(T <i>Femp</i>, Func <i>F</i>) if (doubleInput!T &amp;&amp; is(ReturnType!Func : double));
</big></dt>
<dd>One-sample Kolmogorov-Smirnov test against a reference distribution.
Takes a callable object for the CDF of refernce distribution.
<br><br>
<b>Returns:</b><br>
A TestRes with the Kolmogorov-Smirnov D value and a P value for the
<b>null</b> that <i>Femp</i> is a sample from <i>F</i> against the alternative that it isn't. This
implementation uses a signed D value to indicate the direction of the
difference between distributions.  To get the D value used in standard
notation, simply take the absolute value of this D value.

<br><br>
<font color=red>BUGS:</font><br>
Exact calculation not implemented.  Uses asymptotic approximation.

<br><br>
<b>Examples:</b><br>
<pre class="d_code"><font color=blue>auto</font> stdNormal = parametrize!(normalCDF)(0.0, 1.0);
<font color=blue>auto</font> empirical = [1, 2, 3, 4, 5];
<font color=blue>auto</font> res = <u>ksTest</u>(empirical, stdNormal);
</pre>

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test<br><br>

</dd>
<dt><big><a name="ksTestDestructive"></a>TestRes <u>ksTestDestructive</u>(T, U)(T <i>F</i>, U <i>Fprime</i>) if (isArrayLike!T &amp;&amp; isArrayLike!U);
</big></dt>
<dd>Same as ksTest, except sorts in place, avoiding memory allocations.<br><br>

</dd>
<dt><big><a name="ksTestDestructive.2"></a>TestRes <u>ksTestDestructive</u>(T, Func)(T <i>Femp</i>, Func <i>F</i>) if (isArrayLike!T &amp;&amp; is(ReturnType!Func : double));
</big></dt>
<dd>Ditto.<br><br>

</dd>
<dt><big><a name="runsTest"></a>double <u>runsTest</u>(alias positive = "a &gt; 0", T)(T <i>obs</i>, Alt <i>alt</i> = Alt.twoSided) if (isIterable!T);
</big></dt>
<dd>Wald-wolfowitz or runs test for randomness of the distribution of
elements for which positive() evaluates to <b>true</b>.  For example, given
a sequence of coin flips [H,H,H,H,H,T,T,T,T,T] and a positive() function of
"a == 'H'", this test would determine that the heads are non-randomly
distributed, since they are all at the beginning of <i>obs</i>.  This is done
by counting the number of runs of consecutive elements for which
positive() evaluates to <b>true</b>, and the number of consecutive runs for which
it evaluates to <b>false</b>.  In the example above, we have 2 runs.  These are the
block of 5 consecutive heads at the beginning and the 5 consecutive tails
at the end.
<br><br>
Alternatives are Alt.less, meaning that less runs than expected have been
observed and data for which positive() is <b>true</b> tends to cluster,
Alt.greater, which means that more runs than expected have been observed
and data for which positive() is <b>true</b> tends to not cluster even moreso than
expected by chance, and Alt.twoSided, meaning that elements for which
positive() is <b>true</b> cluster as much as expected by chance.

<br><br>
<font color=red>BUGS:</font><br>
No exact calculation of the P-value.  Asymptotic approximation only.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Runs_test<br><br>

</dd>
<dt><big><a name="RunsTest"></a>struct <u>RunsTest</u>(alias positive = "a &gt; 0", T);
</big></dt>
<dd>Runs test as in runsTest(), except calculates online instead of from stored
array elements.<br><br>

<dl><dt><big><a name="RunsTest.put"></a>void <u>put</u>(T <i>elem</i>);
</big></dt>
<dd><br><br>
</dd>
<dt><big><a name="RunsTest.nRuns"></a>uint <u>nRuns</u>();
</big></dt>
<dd><br><br>
</dd>
<dt><big><a name="RunsTest.p"></a>double <u>p</u>(Alt <i>alt</i> = Alt.twoSided);
</big></dt>
<dd><br><br>
</dd>
</dl>
</dd>
<dt><big><a name="pearsonCorTest"></a>ConfInt <u>pearsonCorTest</u>(T, U)(T <i>range1</i>, U <i>range2</i>, Alt <i>alt</i> = Alt.twoSided, double <i>confLevel</i> = 0.95) if (doubleInput!T &amp;&amp; doubleInput!U);
</big></dt>
<dd>Tests the hypothesis that the Pearson correlation between two ranges is
different from some 0.  Alternatives are Alt.less
(pearsonCor(<i>range1</i>, <i>range2</i>) &lt; 0), Alt.greater (pearsonCor(<i>range1</i>, <i>range2</i>)
 0) and Alt.twoSided (pearsonCor(<i>range1</i>, <i>range2</i>) != 0).
<br><br>
<b>Returns:</b><br>
A ConfInt of the estimated Pearson correlation of the two ranges,
the P-value against the given alternative, and the confidence interval of
the correlation at the level specified by <i>confLevel</i>.

<br><br>
<b>References:</b><br>
<br><br>
<b>http:</b><br>
//en.wikipedia.org/wiki/Pearson_correlation<br><br>

</dd>
<dt><big><a name="pearsonCorTest.2"></a>ConfInt <u>pearsonCorTest</u>()(double <i>cor</i>, double <i>N</i>, Alt <i>alt</i> = Alt.twoSided, double <i>confLevel</i> = 0.95);
</big></dt>
<dd>Same as overload, but uses pre-computed correlation coefficient and sample
size instead of computing them.
<br><br>
<b>Note:</b><br>
This is a template only because of DMD Bug 2972.<br><br>

</dd>
<dt><big><a name="spearmanCorTest"></a>TestRes <u>spearmanCorTest</u>(T, U)(T <i>range1</i>, U <i>range2</i>, Alt <i>alt</i> = Alt.twoSided) if (isInputRange!T &amp;&amp; isInputRange!U &amp;&amp; is(typeof(<i>range1</i>.front &lt; <i>range1</i>.front) == bool) &amp;&amp; is(typeof(<i>range2</i>.front &lt; <i>range2</i>.front) == bool));
</big></dt>
<dd>Tests the hypothesis that the Spearman correlation between two ranges is
different from some 0.  Alternatives are
Alt.less (spearmanCor(<i>range1</i>, <i>range2</i>) &lt; 0), Alt.greater (spearmanCor(<i>range1</i>, <i>range2</i>)
&gt; 0) and Alt.twoSided (spearmanCor(<i>range1</i>, <i>range2</i>) != 0).
<br><br>
<b>Returns:</b><br>
A TestRes containing the Spearman correlation coefficient and
the P-value for the given alternative.

<br><br>
<font color=red>BUGS:</font><br>
Exact P-value computation not yet implemented.  Uses asymptotic
approximation only.  This is good enough for most practical purposes given
reasonably large N, but is not perfectly accurate.  Not valid for data with
very large amounts of ties.<br><br>

</dd>
<dt><big><a name="kendallCorTest"></a>TestRes <u>kendallCorTest</u>(T, U)(T <i>range1</i>, U <i>range2</i>, Alt <i>alt</i> = Alt.twoSided, uint <i>exactThresh</i> = 50) if (isInputRange!T &amp;&amp; isInputRange!U);
</big></dt>
<dd>Tests the hypothesis that the Kendall Tau-b between two ranges is
different from 0.  Alternatives are Alt.less (kendallCor(<i>range1</i>, <i>range2</i>) &lt; 0),
Alt.greater (kendallCor(<i>range1</i>, <i>range2</i>) &gt; 0) and Alt.twoSided
(kendallCor(<i>range1</i>, <i>range2</i>) != 0).
<br><br>
<i>exactThresh</i> controls the maximum length of the range for which exact P-value
computation is used.  The default is 50.  Exact calculation is never used
when ties are present because it is not computationally feasible.
Do not set this higher than 100, as it will be very slow
and the asymptotic approximation is pretty good at even a fraction of this
size.

<br><br>
<b>Note:</b><br>
As an optimization, when a range is a SortedRange with predicate "a &lt; b",
it is assumed already sorted and not sorted a second time by this function.
This is useful when applying this function multiple times with one of the
arguments the same every time.

<br><br>
<b>Returns:</b><br>
A TestRes containing the Kendall correlation coefficient and
the P-value for the given alternative.

<br><br>
<b>References:</b><br>
StackOverflow Question 948341 (http://stackoverflow.com/questions/948341)
<br><br>

The Variance of Tau When Both Rankings Contain Ties.  M.G. Kendall.
Biometrika, Vol 34, No. 3/4 (Dec., 1947), pp. 297-298<br><br>

</dd>
<dt><big><a name="dAgostinoK"></a>TestRes <u>dAgostinoK</u>(T)(T <i>range</i>) if (doubleIterable!T);
</big></dt>
<dd>A test for normality of the distribution of a <i>range</i> of values.  Based on
 the assumption that normally distributed values will have a sample skewness
 and sample kurtosis very close to zero.
<br><br>
<b>Returns:</b><br>
A TestRes with the K statistic, which is Chi-Square distributed
 with 2 degrees of freedom under the <b>null</b>, and the P-value for the alternative
 that the data has skewness and kurtosis not equal to zero against the <b>null</b>
 that skewness and kurtosis are near zero.  A normal distribution always has
 skewness and kurtosis that converge to zero as sample size goes to infinity.

<br><br>
<b>Notes:</b><br>
Contrary to popular belief, tests for normality should usually
 not be used to deterimine whether T-tests are valid.  If the sample size is
 large, T-tests are valid regardless of the distribution due to the central
 limit theorem.  If the sample size is small, a test for normality will
 likely not be very powerful, and a priori knowledge or simple inspection
 of the data is often a better idea.

<br><br>
<b>References:</b><br>
D'Agostino, Ralph B., Albert Belanger, and Ralph B. D'Agostino, Jr.
 "A Suggestion for Using Powerful and Informative Tests of Normality",
 The American Statistician, Vol. 44, No. 4. (Nov., 1990), pp. 316-321.<br><br>

</dd>
<dt><big><a name="fishersMethod"></a>TestRes <u>fishersMethod</u>(R)(R <i>pVals</i>) if (doubleInput!R);
</big></dt>
<dd>Fisher's method of meta-analyzing a set of P-values to determine whether
 there are more significant results than would be expected by chance.
 Based on a chi-square statistic for the sum of the logs of the P-values.
<br><br>
<b>Returns:</b><br>
A TestRes containing the chi-square statistic and a P-value for
 the alternative hypothesis that more small P-values than would be expected
 by chance are present against the alternative that the distribution of
 P-values is uniform or enriched for large P-values.

<br><br>
<b>References:</b><br>
Fisher, R. A. (1948) "Combining independent tests of
 significance", American Statistician, vol. 2, issue 5, page 30.
 (In response to Question 14)<br><br>

</dd>
<dt><big><a name="Dependency"></a>enum <u>Dependency</u>: bool;
</big></dt>
<dd>For falseDiscoveryRate.<br><br>

<dl><dt><big><a name="Dependency.yes"></a><u>yes</u></big></dt>
<dd>Assume that dependency among hypotheses may exist.  (More conservative,
    Benjamini-Yekutieli procedure.)<br><br>

</dd>
<dt><big><a name="Dependency.no"></a><u>no</u></big></dt>
<dd>Assume hypotheses are independent.  (Less conservative, Benjamine-
    Hochberg procedure.)<br><br>

</dd>
</dl>
</dd>
<dt><big><a name="falseDiscoveryRate"></a>float[] <u>falseDiscoveryRate</u>(T)(T <i>pVals</i>, Dependency <i>dep</i> = Dependency.no) if (doubleInput!T);
</big></dt>
<dd>Computes the <b>false</b> discovery rate statistic given a list of
p-values, according to Benjamini and Hochberg (1995) (independent) or
Benjamini and Yekutieli (2001) (dependent).  The Dependency parameter
controls whether hypotheses are assumed to be independent, or whether
the more conservative assumption that they are correlated must be made.
<br><br>
<b>Returns:</b><br>
An array of adjusted P-values with indices corresponding to the order of
the P-values in the input data.

<br><br>
<b>References:</b><br>
Benjamini, Y., and Hochberg, Y. (1995). Controlling the <b>false</b> discovery rate:
a practical and powerful approach to multiple testing. Journal of the Royal
Statistical Society Series B, 57, 289-200
<br><br>

Benjamini, Y., and Yekutieli, D. (2001). The control of the <b>false</b> discovery
rate in multiple testing under dependency. Annals of Statistics 29, 1165-1188.<br><br>

</dd>
<dt><big><a name="hochberg"></a>float[] <u>hochberg</u>(T)(T <i>pVals</i>) if (doubleInput!T);
</big></dt>
<dd>Uses the Hochberg procedure to control the familywise error rate assuming
 that hypothesis tests are independent.  This is more powerful than
 Holm-Bonferroni correction, but requires the independence assumption.
<br><br>
<b>Returns:</b><br>
An array of adjusted P-values with indices corresponding to the order of
 the P-values in the input data.

<br><br>
<b>References:</b><br>
Hochberg, Y. (1988). A sharper Bonferroni procedure for multiple tests of
 significance. Biometrika, 75, 800-803.<br><br>

</dd>
<dt><big><a name="holmBonferroni"></a>float[] <u>holmBonferroni</u>(T)(T <i>pVals</i>) if (doubleInput!T);
</big></dt>
<dd>Uses the Holm-Bonferroni method to adjust a set of P-values in a way that
 controls the familywise error rate (The probability of making at least one
 Type I error).  This is basically a less conservative version of
 Bonferroni correction that is still valid for arbitrary assumptions and
 controls the familywise error rate.  Therefore, there aren't too many good
 reasons to use regular Bonferroni correction instead.
<br><br>
<b>Returns:</b><br>
An array of adjusted P-values with indices corresponding to the order of
 the P-values in the input data.

<br><br>
<b>References:</b><br>
Holm, S. (1979). A simple sequentially rejective multiple test procedure.
 Scandinavian Journal of Statistics, 6, 65-70.<br><br>

</dd>
</dl>

        <hr><small>Page generated by <a href="http://dlang.org/ddoc.html">Ddoc</a>. </small>
        </body></html>
